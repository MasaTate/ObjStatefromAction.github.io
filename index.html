<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning object states from actions via large language models.">
  <meta name="keywords" content="LLM, object states, actions">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning Object States from Actions via Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Object States from Actions via Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://masatate.github.io/">Masatoshi Tateno</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://artilects.net/">Takuma Yagi</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://rfuruta.github.io/">Ryosuke Furuta</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/ut-vision.org/ysato/">Yoichi Sato</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Tokyo,</span>
            <span class="author-block"><sup>2</sup>National Institute of Advanced Industrial Science and Technology (AIST) </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.01090"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-light">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code & Data</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <image src="./static/images/task.png" height="100%">
        <h2 class="subtitle has-text-centered">
          We formulate the object state recognition task as a frame-wise multi-label classification problem.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/images/teaser_crop.mp4"
                    type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered">
            We propose a novel method to learn object states from actions using large language models.
        </h2>
      </div>
    </div>
  </section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Temporally localizing the presence of object states in videos is crucial in understanding
              human activities beyond actions and objects. This task has suffered from a lack of training data due to object states' inherent ambiguity and variety. 
            </p>
            <p>
                To avoid exhaustive annotation, learning from transcribed narrations in instructional videos would be intriguing. 
                However, object states are less described in narrations 
                compared to actions, making them less effective. 
                In this work, we propose to extract the object state information from action information 
                included in narrations, using large language models (LLMs). Our observation is that LLMs 
                include world knowledge on the relationship between actions and their resulting 
                object states, and can infer the presence of object states from past action sequences. 
                The proposed LLM-based framework offers flexibility to generate plausible pseudo-object 
                state labels against arbitrary categories. 
            </p>
            <p>
                We evaluate our method with our newly collected Multiple Object States Transition (MOST) 
                dataset including dense temporal annotation of 60 object state categories. 
                Our model trained by the generated pseudo-labels demonstrates significant improvement of 
                over 29% in mAP against strong zero-shot vision-language models, showing the effectiveness 
                of explicitly extracting object state information from actions through LLMs.
            </p>
          </div>
        </div>
    </div>
      <!--/ Abstract. -->
  
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/0-4A3SktAe8?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->

      <!-- Dataset. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Multiple Object States Transition (MOST) Datasets</h2>
           <ul class="has-text-left">
            <li>More than 150 minutes of videos.</li>
            <li>Six object categories (apple, egg, flour, shirt, tire, wire) from various domains.</li>
            <li>Interval annotation for about 10 states per object.</li>
           </ul>
          <div class="content has-text-justified">
            <video id="teaser" autoplay muted loop playsinline height="100%">
                <source src="./static/images/MOST_videos.mp4"
                        type="video/mp4">
              </video>
          </div>
        </div>
    </div>
      <!--/ Dataset. -->

    </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{tateno2024learning,
title={Learning Object States from Actions via Large Language Models},
author={Tateno, Masatoshi and Yagi, Takuma and Furuta, Ryosuke and Sato, Yoichi},
journal={arXiv preprint arXiv:2405.01090},
year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template was borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>