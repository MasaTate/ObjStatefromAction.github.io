<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning multiple object states from actions via large language models.">
  <meta name="keywords" content="LLM, object states, actions">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning Multiple Object States from Actions via Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Multiple Object States from Actions via Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://masatate.github.io/">Masatoshi Tateno</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://artilects.net/">Takuma Yagi</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://rfuruta.github.io/">Ryosuke Furuta</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/ut-vision.org/ysato/">Yoichi Sato</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Tokyo,</span>
            <span class="author-block"><sup>2</sup>National Institute of Advanced Industrial Science and Technology (AIST) </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.01090"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=0-4A3SktAe8"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MasaTate/ObjectStatefromAction"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code & Data</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="container is-max-desktop">
  <div class="hero-body">
    <h2 class="subtitle has-text-centered">
      <p style="color: red;">Our paper has been accepted for WACV 2025 üéâ</p>
    </h2>
  </div>
</div>

<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <image src="./static/images/task.png" height="90%">
        <h2 class="subtitle has-text-centered">
          We formulate the object state recognition task as a frame-wise multi-label classification problem.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="90%">
            <source src="./static/images/teaser_crop.mp4"
                    type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered">
            We propose a novel method to learn object states from actions using large language models.
        </h2>
      </div>
    </div>
  </section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recognizing the states of objects in a video is crucial in understanding the scene beyond actions and objects.
              For instance, an egg can be ‚Äúraw,‚Äù ‚Äúcracked,‚Äù and ‚Äúwhisked‚Äù while cooking an omelet, and these states can coexist simultaneously (an egg can be both ‚Äúraw‚Äù and ‚Äúwhisked‚Äù).
              However, most existing research assumes single object state change (e.g., uncracked to cracked), overlooking the coexisting nature of multiple object states and the influence of past states on the current state. 
            </p>
            <p>
              We formulate object state recognition as a multi-label classification task that explicitly handles multiple states. We then propose to learn multiple object states from narrated videos by leveraging large language models (LLMs) to generate pseudo-labels from the transcribed narrations, capturing the influence of past states.
              The challenge is that narrations mostly describe human actions in the video but rarely explain object states.
              Therefore, we use LLMs‚Äô knowledge of the relationship between actions and states to derive the missing object states.
              We further accumulate the derived object states to consider the past state contexts to infer current object state pseudo-labels. 
            </p>
            <p>
              We newly collect the Multiple Object States Transition (MOST) dataset, which includes manual multi-label annotation for evaluation purposes, covering 60 object states across six object categories.
              Experimental results show that our model trained on LLM-generated pseudo-labels significantly outperforms strong vision-language models, demonstrating the effectiveness of our pseudo-labeling framework that considers past context via LLMs.
            </p>

            <!-- <p>
              Temporally localizing the presence of object states in videos is crucial in understanding
              human activities beyond actions and objects. This task has suffered from a lack of training data due to object states' inherent ambiguity and variety. 
            </p>
            <p>
                To avoid exhaustive annotation, learning from transcribed narrations in instructional videos would be intriguing. 
                However, object states are less described in narrations 
                compared to actions, making them less effective. 
                In this work, we propose to extract the object state information from action information 
                included in narrations, using large language models (LLMs). Our observation is that LLMs 
                include world knowledge on the relationship between actions and their resulting 
                object states, and can infer the presence of object states from past action sequences. 
                The proposed LLM-based framework offers flexibility to generate plausible pseudo-object 
                state labels against arbitrary categories. 
            </p>
            <p>
                We evaluate our method with our newly collected Multiple Object States Transition (MOST) 
                dataset including dense temporal annotation of 60 object state categories. 
                Our model trained by the generated pseudo-labels demonstrates significant improvement of 
                over 29% in mAP against strong zero-shot vision-language models, showing the effectiveness 
                of explicitly extracting object state information from actions through LLMs.
            </p> -->


          </div>
        </div>
    </div>
      <!--/ Abstract. -->
  
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/0-4A3SktAe8?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->

      <!-- Dataset. -->
      <div class="columns is-centered has-text-centered", style="margin-top: 40px;">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Multiple Object States Transition (MOST) Datasets</h2>
          <div class="content has-text-justified">
            <p>
                We created a new evaluation dataset on temporally localizing the presence of object states.
                The videos include complicated transition between different states, which are annotated with dense temporal intervals.
                The dataset covers various object states including those that are not necessarily associated with actions (e.g., straight, dry, smooth).
            </p>
          </div>
           <div class="has-text-left">
            <li>More than 150 minutes of videos.</li>
            <li>Six object categories (apple, egg, flour, shirt, tire, wire) from various domains.</li>
            <li>Interval annotation for about 10 states per object.</li>
        </div>
          <div class="content has-text-justified" style="margin-top: 20px;">
            <video id="teaser" autoplay muted loop playsinline height="90%">
                <source src="./static/images/MOST_videos.mp4"
                        type="video/mp4">
              </video>
          </div>
        </div>
    </div>
      <!--/ Dataset. -->

      <!-- Qualitative Results-->
      <div class="container is-max-desktop" style="margin-top: 20px">
        <h2 class="title is-3">Qualitative Results</h2>
        <h3 class="title is-4">Videos</h3>
        <div class="columns is-centered">
            <div class="column">
                <p> Target Object: Apple</p>
                <video id="teaser" autoplay muted loop playsinline height="90%">
                    <source src="./static/images/On1FA92D8o_clip_web.mp4"
                            type="video/mp4">
                </video>
            </div>

            <div class="column">
                <p> Target Object: Shirt</p>
                <video id="teaser" autoplay muted loop playsinline height="90%">
                    <source src="./static/images/CYpSr8ND9sE_clip_web.mp4"
                            type="video/mp4">
                </video>
            </div>
        </div>
        
        <div class="column is-four-fifths">
            <h3 class="title is-4">Frames</h3>
            <p>Target Object: Apple</p>
            <image src="./static/images/fq_apple.jpg" height="100%"> 
            <p>Target Object: Egg</p>
            <image src="./static/images/fq_egg.jpg" height="100%"> 
            <p>Target Object: Flour</p>
            <image src="./static/images/fq_flour.jpg" height="100%">
            <p>Target Object: Shirt</p>
            <image src="./static/images/fq_shirt.jpg" height="100%">
            <p>Target Object: Tire</p>
            <image src="./static/images/fq_tire.jpg" height="100%">
            <p>Target Object: Wire</p>
            <image src="./static/images/fq_wire.jpg" height="100%">
        </div>
        </div>
    </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{tateno2024learning,
title={Learning Object States from Actions via Large Language Models},
author={Tateno, Masatoshi and Yagi, Takuma and Furuta, Ryosuke and Sato, Yoichi},
journal={arXiv preprint arXiv:2405.01090},
year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2405.01090">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/MasaTate/ObjectStatefromAction" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template was borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>